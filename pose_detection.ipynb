{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import math\n",
    "import cv2\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Mediapipe and drawing landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the mediapipe model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.3)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_and_get_landmarks(image, pose):\n",
    "    \"\"\"Places landmarks on an image.\"\"\"\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Identify all landmarks\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(imageRGB)\n",
    "    \n",
    "    # Retrieve the height and width of the input image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Append all landmarks and draw them on the OUTPUT image\n",
    "    landmarks = []\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image=output_image, landmark_list=results.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.append((int(landmark.x * width), int(landmark.y * height),\n",
    "                                  (landmark.z * width)))\n",
    "    \n",
    "    return output_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating angles between landmarks and detecting specific poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(landmark1, landmark2, landmark3):\n",
    "    \"\"\"Calculate the angle between 3 landmarks.\"\"\"\n",
    "    # Get the required landmarks coordinates.\n",
    "    x1, y1, _ = landmark1\n",
    "    x2, y2, _ = landmark2\n",
    "    x3, y3, _ = landmark3\n",
    " \n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    \n",
    "    # Make the angle positive\n",
    "    return abs(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_salute(shoulder, elbow, wrist, frame, display_frame=False):\n",
    "    \"\"\"Checks to see if the person is saluting.\"\"\"\n",
    "    is_saluting = True if 25 < calculate_angle(shoulder, elbow, wrist) < 40 else False\n",
    "    if is_saluting and display_frame:\n",
    "        cv2.putText(frame, \"Saluting!\", (10, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)\n",
    "    return is_saluting\n",
    "\n",
    "def check_right_arm_up(shoulder, elbow, wrist, hip, frame, display_frame=False):\n",
    "    \"\"\"Checks to see if the person has their right arm at a 90 degree angle to their body.\"\"\"\n",
    "    is_arm_up = True if 85 < calculate_angle(elbow, shoulder, hip) < 95 else False\n",
    "    if not is_arm_up:\n",
    "        return False\n",
    "    \n",
    "    is_arm_flat = True if 175 < calculate_angle(wrist, elbow, shoulder) < 185 else False\n",
    "    if display_frame:\n",
    "        cv2.putText(frame, \"Right T-Pose\", (10, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)\n",
    "    return is_arm_flat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pyautogui for movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_forward():\n",
    "    \"\"\"Press the w key on the user's laptop.\"\"\"\n",
    "    pyautogui.keyDown('W')\n",
    "    \n",
    "\n",
    "def move_right():\n",
    "    \"\"\"Press the d key on the user's laptop.\"\"\"\n",
    "    pyautogui.keyDown('D')\n",
    "    \n",
    "\n",
    "def move_left():\n",
    "    \"\"\"Press the a key on the user's laptop.\"\"\"\n",
    "    pyautogui.keyDown('A')\n",
    "    \n",
    "\n",
    "def stop_movement():\n",
    "    \"\"\"Stops all movement.\"\"\"\n",
    "    pyautogui.keyUp('W')\n",
    "    pyautogui.keyUp('D')\n",
    "    pyautogui.keyUp('A')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting everything together with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m         move_left()\n\u001b[0;32m     44\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m         stop_movement()\n\u001b[0;32m     47\u001b[0m \u001b[39m# Display the frame.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m# +++++++++++++++++++++++++++++++ UNCOMMENT THIS TO SEE THE VIDEO +++++++++++++++++++++++++++++++++\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m# cv2.imshow('Pose Detection', frame)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[39m# Check to see if the user pressed 'esc'\u001b[39;00m\n\u001b[0;32m     52\u001b[0m k \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mstop_movement\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Stops all movement.\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m pyautogui\u001b[39m.\u001b[39mkeyUp(\u001b[39m'\u001b[39m\u001b[39mW\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m pyautogui\u001b[39m.\u001b[39;49mkeyUp(\u001b[39m'\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m pyautogui\u001b[39m.\u001b[39mkeyUp(\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\srika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyautogui\\__init__.py:595\u001b[0m, in \u001b[0;36m_genericPyAutoGUIChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m failSafeCheck()\n\u001b[0;32m    594\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 595\u001b[0m _handlePause(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    596\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\srika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyautogui\\__init__.py:639\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    638\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 639\u001b[0m     time\u001b[39m.\u001b[39msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, model_complexity=1)\n",
    " \n",
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# +++++++++++++++++++++++++++++++ UNCOMMENT THIS TO SEE THE VIDEO +++++++++++++++++++++++++++++++++\n",
    "# cv2.namedWindow('Pose Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Initialize a variable to store the time of the previous frame.\n",
    "time1 = 0\n",
    " \n",
    "# Iterate until the video is accessed successfully.\n",
    "while video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = video.read()\n",
    "    \n",
    "    # Check if frame is not read properly.\n",
    "    if not ok:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Adjust the frame\n",
    "    frame_height, frame_width, _ =  frame.shape\n",
    "    frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "    \n",
    "    # Perform Pose landmark detection.\n",
    "    frame, landmarks = draw_and_get_landmarks(frame, pose_video)\n",
    "\n",
    "    if landmarks:\n",
    "        # Everything is mirrored here so I have to pass it in weirdly...\n",
    "        is_saluting_right = check_salute(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value], landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value], \n",
    "                                   landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value], frame)\n",
    "        is_saluting_left =  check_salute(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value], landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value], \n",
    "                                   landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value], frame)\n",
    "        is_right_arm_up = check_right_arm_up(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value], landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value], \n",
    "                                   landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value], landmarks[mp_pose.PoseLandmark.LEFT_HIP.value], frame)\n",
    "        \n",
    "        if is_saluting_right:\n",
    "            move_right()\n",
    "        elif is_saluting_left:\n",
    "            move_left()\n",
    "        else:\n",
    "            stop_movement()\n",
    "    \n",
    "    # Display the frame.\n",
    "    # +++++++++++++++++++++++++++++++ UNCOMMENT THIS TO SEE THE VIDEO +++++++++++++++++++++++++++++++++\n",
    "    # cv2.imshow('Pose Detection', frame)\n",
    "    \n",
    "    # Check to see if the user pressed 'esc'\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if(k == 27):\n",
    "        break\n",
    " \n",
    "# Clean Up\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ending the Demo (Optional)\n",
    "TO end the demo when you have to gui window, just stop the jupyter cell from running. Then your camera should still be in use so to terminate that process, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Up\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
